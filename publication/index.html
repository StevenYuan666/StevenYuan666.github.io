<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <title>Ye Yuan | Publications</title>
    <meta name="description" content="A beautiful Jekyll theme for academics">
    <link rel="icon" type="image/png" href="/assets/img/mcgill.png">

    <!-- Fonts and Icons -->
    <link rel="stylesheet" type="text/css"
        href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons" />

    <!-- CSS Files -->
    <link rel="stylesheet" href="/assets/css/all.min.css">
    <link rel="stylesheet" href="/assets/css/academicons.min.css">
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <link rel="canonical" href="/publication/">
</head>

<body>
    <!-- Header -->
    <nav id="navbar" class="navbar fixed-top navbar-expand-md grey lighten-5 z-depth-1 navbar-light">
        <div class="container-fluid p-0">

            <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Ye</span>
                Yuan</a>
            <div class="row ml-1 ml-sm-0">
                <span class="contact-icon text-center" style="line-height: 1em;"><a href="mailto:ye.yuan3@mail.mcgill.ca?subject=Hello%20Ye%20Yuan&body=Dear%20Ye,%0A%0AI%20would%20like%20to%20contact%20you%20regarding...">
                    <i class="fa fa-envelope-square gm-icon"></i>
                </a>                
                    <a href="https://scholar.google.ca/citations?user=lemEc74AAAAJ&hl=en&oi=sra" target="_blank"
                        title="Google Scholar"><i class="ai ai-google-scholar-square gs-icon"></i></a>
                    <a href="https://github.com/StevenYuan666" target="_blank" title="GitHub"><i
                            class="fab fa-github-square gh-icon"></i></a>
                    <a href="https://www.linkedin.com/in/ye-yuan-21bb351b5/" target="_blank" title="LinkedIn"><i
                            class="fab fa-linkedin li-icon"></i></a>
                    <a href="https://x.com/StevenYuan666" target="_blank" title="Twitter"><i
                            class="fab fa-twitter-square tw-icon"></i></a>
                    <a href="/assets/img/weixin.JPG" target="_blank" title="WeChat"><i class="fab fa-weixin"></i></a>
                </span>
            </div>

            <button class="navbar-toggler ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav"
                aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse text-right" id="navbarNav">
                <ul class="navbar-nav ml-auto flex-nowrap">
                    <li class="nav-item ">
                        <a class="nav-link" href="/">
                            About

                        </a>
                    </li>

                    <li class="nav-item "><a class="nav-link" href="/experience/">Experience</a></li>
                    <li class="nav-item "><a class="nav-link" href="/award/">Award</a></li>
                    <li class="nav-item navbar-active font-weight-bold"><a class="nav-link"
                            href="/publication/">Publication<span class="sr-only">(current)</span></a></li>
                    <li class="nav-item "><a class="nav-link" href="/talk/">Talk</a></li>
                    <li class="nav-item "><a class="nav-link" href="/service/">Service</a></li>
                    <li class="nav-item "><a class="nav-link" href="/misc/">Misc</a></li>

                </ul>
            </div>
        </div>
    </nav>

    <!-- Scrolling Progress Bar -->
    <progress id="progress" value="0">
        <div class="progress-container">
            <span class="progress-bar"></span>
        </div>
    </progress>

    <!-- Content -->
    <div class="content">

        <h1>Publications</h1>
        <h6>
            <nobr><em>*</em></nobr>
            Equal contribution.
        </h6>
        <p><br /></p>

        <h2>arXiv Preprint</h2>

        <div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
            <div class="col-sm-1 mt-2 p-0 pr-1">
                <h3 class="bibliography-year">arXiv</h3>
            </div>
            <div class="col-sm-11 p-0">
                <ol class="bibliography">

                    <li>
                        <div class="row m-0 mt-3 p-0">
                            <div class="col-sm-1 p-0 abbr">
                                <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;"
                                    href="https://arxiv.org/pdf/2505.14906" target="_blank">
                                    arXiv
                                </a>
                            </div>
                            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                                <div id="Yuan2025Understanding" class="col p-0">
                                    <h5 class="title mb-0">Understanding 6G through Language Models: A Case Study on LLM-aided Structured Entity Extraction in Telecom Domain
                                    </h5>
                                    <div class="author">
                                        <nobr><em>Ye Yuan</em>,</nobr>
                                        <nobr>Haolun Wu,</nobr>
                                        <nobr>Hao Zhou,</nobr>
                                        <nobr>Xue Liu,</nobr>
                                        <nobr>Hao Chen,</nobr>
                                        <nobr>Yan Xin,</nobr>
                                        and
                                        <nobr>Charlie Zhang.</nobr>

                                    </div>

                                    <div>
                                        <p class="periodical font-italic">
                                            arXiv:2505.14906.
                                        </p>

                                    </div>


                                    <div class="col p-0">

                                        <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                            href="#Yuan2025Understanding-abstract" role="button" aria-expanded="false"
                                            aria-controls="Yuan2025Understanding-abstract">Abstract</a>

                                        <a class="badge grey waves-effect font-weight-light mr-1"
                                            href="/assets/pdf/telecom/2505.14906v1.pdf" target="_blank">PDF</a>

                                        <!--                                    <a class="badge grey waves-effect font-weight-light mr-1"-->
                                        <!--                                       href="/assets/pdf/lumos/bobu2020HRIpioneers_poster.pdf"-->
                                        <!--                                       target="_blank">Poster</a>-->

                                    </div>


                                    <div class="col mt-2 p-0">
                                        <div id="Yuan2025Understanding-abstract" class="collapse">
                                            <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                                Knowledge understanding is a foundational part of envisioned 6G networks to advance network intelligence and AI-native network architectures. In this paradigm, information extraction plays a pivotal role in transforming fragmented telecom knowledge into well-structured formats, empowering diverse AI models to better understand network terminologies. This work proposes a novel language model-based information extraction technique, aiming to extract structured entities from the telecom context. The proposed telecom structured entity extraction (TeleSEE) technique applies a token-efficient representation method to predict entity types and attribute keys, aiming to save the number of output tokens and improve prediction accuracy. Meanwhile, TeleSEE involves a hierarchical parallel decoding method, improving the standard encoder-decoder architecture by integrating additional prompting and decoding strategies into entity extraction tasks. In addition, to better evaluate the performance of the proposed technique in the telecom domain, we further designed a dataset named 6GTech, including 2390 sentences and 23747 words from more than 100 6G-related technical publications. Finally, the experiment shows that the proposed TeleSEE method achieves higher accuracy than other baseline techniques, and also presents 5 to 9 times higher sample processing speed.    
                                            </div>
                                        </div>
                                    </div>

                                </div>
                            </div>
                        </div>
                    </li>

                    <li>
                        <div class="row m-0 mt-3 p-0">
                            <div class="col-sm-1 p-0 abbr">
                                <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;"
                                    href="https://arxiv.org/abs/2503.17286" target="_blank">
                                    arXiv
                                </a>
                            </div>
                            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                                <div id="Kim2025Offline" class="col p-0">
                                    <h5 class="title mb-0">Offline Model-Based Optimization: Comprehensive Review
                                    </h5>
                                    <div class="author">
                                        <nobr>Minsu Kim*,</nobr>
                                        <nobr>Jiayao Gu*,</nobr>
                                        <nobr><em>Ye Yuan</em>,</nobr>
                                        <nobr>Taeyoung Yun,</nobr>
                                        <nobr>Zixuan Liu,</nobr>
                                        <nobr>Yoshua Bengio,</nobr>
                                        and
                                        <nobr>Can Chen.</nobr>

                                    </div>

                                    <div>
                                        <p class="periodical font-italic">
                                            arXiv:2503.17286.
                                        </p>

                                    </div>


                                    <div class="col p-0">

                                        <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                            href="#Kim2025Offline-abstract" role="button" aria-expanded="false"
                                            aria-controls="Kim2025Offline-abstract">Abstract</a>

                                        <a class="badge grey waves-effect font-weight-light mr-1"
                                            href="/assets/pdf/mbo/MBO_survey_2025.pdf" target="_blank">PDF</a>

                                    </div>


                                    <div class="col mt-2 p-0">
                                        <div id="Kim2025Offline-abstract" class="collapse">
                                            <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                                Offline optimization is a fundamental challenge in science and engineering, where the goal is to optimize black-box functions using only offline datasets. This setting is particularly relevant when querying the objective function is prohibitively expensive or infeasible, with applications spanning protein engineering, material discovery, neural architecture search, and beyond. The main difficulty lies in accurately estimating the objective landscape beyond the available data, where extrapolations are fraught with significant epistemic uncertainty. This uncertainty can lead to objective hacking(reward hacking), exploiting model inaccuracies in unseen regions, or other spurious optimizations that yield misleadingly high performance estimates outside the training distribution. Recent advances in model-based optimization(MBO) have harnessed the generalization capabilities of deep neural networks to develop offline-specific surrogate and generative models. Trained with carefully designed strategies, these models are more robust against out-of-distribution issues, facilitating the discovery of improved designs. Despite its growing impact in accelerating scientific discovery, the field lacks a comprehensive review. To bridge this gap, we present the first thorough review of offline MBO. We begin by formalizing the problem for both single-objective and multi-objective settings and by reviewing recent benchmarks and evaluation metrics. We then categorize existing approaches into two key areas: surrogate modeling, which emphasizes accurate function approximation in out-of-distribution regions, and generative modeling, which explores high-dimensional design spaces to identify high-performing designs. Finally, we examine the key challenges and propose promising directions for advancement in this rapidly evolving field including safe control of superintelligent systems.    
                                            </div>
                                        </div>
                                    </div>

                                </div>
                            </div>
                        </div>
                    </li>

                    <li>
                        <div class="row m-0 mt-3 p-0">
                            <div class="col-sm-1 p-0 abbr">
                                <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;"
                                    href="https://arxiv.org/pdf/2408.00214" target="_blank">
                                    arXiv
                                </a>
                            </div>
                            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                                <div id="Zhou2024PowerControl" class="col p-0">
                                    <h5 class="title mb-0">Large Language Model (LLM)-Enabled In-Context Learning for Wireless Network Optimization: A Case Study of Power Control
                                    </h5>
                                    <div class="author">
                                        <nobr>Hao Zhou,</nobr>
                                        <nobr>Chengming Hu,</nobr>
                                        <nobr>Dun Yuan,</nobr>
                                        <nobr><em>Ye Yuan</em>,</nobr>
                                        <nobr>Di Wu,</nobr>
                                        <nobr>Xue Liu,</nobr>
                                        and
                                        <nobr>Charlie Zhang.</nobr>

                                    </div>

                                    <div>
                                        <p class="periodical font-italic">
                                            arXiv:2408.00214.
                                        </p>

                                    </div>


                                    <div class="col p-0">

                                        <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                            href="#Zhou2024PowerControl-abstract" role="button" aria-expanded="false"
                                            aria-controls="Zhou2024PowerControl-abstract">Abstract</a>

                                        <a class="badge grey waves-effect font-weight-light mr-1"
                                            href="/assets/pdf/telecom/2408.00214v1.pdf" target="_blank">PDF</a>

                                        <!--                                    <a class="badge grey waves-effect font-weight-light mr-1"-->
                                        <!--                                       href="/assets/pdf/lumos/bobu2020HRIpioneers_poster.pdf"-->
                                        <!--                                       target="_blank">Poster</a>-->

                                    </div>


                                    <div class="col mt-2 p-0">
                                        <div id="Zhou2024PowerControl-abstract" class="collapse">
                                            <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                                Large language model (LLM) has recently been considered a promising technique for many fields. This work explores LLM-based wireless network optimization via in-context learning. To showcase the potential of LLM technologies, we consider the base station (BS) power control as a case study, a fundamental but crucial technique that is widely investigated in wireless networks. Different from existing machine learning (ML) methods, our proposed in-context learning algorithm relies on LLM's inference capabilities. It avoids the complexity of tedious model training and hyper-parameter fine-tuning, which is a well-known bottleneck of many ML algorithms. Specifically, the proposed algorithm first describes the target task via formatted natural language, and then designs the in-context learning framework and demonstration examples. After that, it considers two cases, namely discrete-state and continuous-state problems, and proposes state-based and ranking-based methods to select appropriate examples for these two cases, respectively. Finally, the simulations demonstrate that the proposed algorithm can achieve comparable performance as conventional deep reinforcement learning (DRL) techniques without dedicated model training or fine-tuning. Such an efficient and low-complexity approach has great potential for future wireless network optimization.
                                            </div>
                                        </div>
                                    </div>

                                </div>
                            </div>
                        </div>
                    </li>

                    <li>
                        <div class="row m-0 mt-3 p-0">
                            <div class="col-sm-1 p-0 abbr">
                                <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;"
                                    href="https://arxiv.org/abs/2407.13193" target="_blank">
                                    arXiv
                                </a>
                            </div>
                            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                                <div id="Wu2024_RAG" class="col p-0">
                                    <h5 class="title mb-0">Retrieval-Augmented Generation for Natural Language
                                        Processing: A Survey

                                    </h5>
                                    <div class="author">
                                        <nobr>Shangyu Wu,</nobr>
                                        <nobr>Ying Xiong,</nobr>
                                        <nobr>Yufei Cui,</nobr>
                                        <nobr>Haolun Wu,</nobr>
                                        <nobr>Can Chen,</nobr>
                                        <nobr><em>Ye Yuan</em>,</nobr>
                                        <nobr>Lianming Huang,</nobr>
                                        <nobr>Xue Liu,</nobr>
                                        <nobr>Tei-Wei Kuo,</nobr>
                                        <nobr> Nan Guan,</nobr>
                                        and
                                        <nobr>Chun Jason Xue.</nobr>
                                    </div>

                                    <div>
                                        <p class="periodical font-italic">
                                            arXiv:2407.13193.
                                        </p>
                                    </div>

                                    <div class="col p-0">

                                        <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                            href="#Wu2024_RAG-abstract" role="button" aria-expanded="false"
                                            aria-controls="Wu2024_RAG-abstract">Abstract</a>

                                        <a class="badge grey waves-effect font-weight-light mr-1"
                                            href="/assets/pdf/nlp/RAG_survey_2024.pdf" target="_blank">PDF</a>

                                        <!--                                    <a class="badge grey waves-effect font-weight-light mr-1"-->
                                        <!--                                       href="/assets/pdf/lumos/bobu2020HRIpioneers_poster.pdf"-->
                                        <!--                                       target="_blank">Poster</a>-->

                                    </div>


                                    <div class="col mt-2 p-0">
                                        <div id="Wu2024_RAG-abstract" class="collapse">
                                            <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                                Large language models (LLMs) have demonstrated great success in various
                                                fields, benefiting from their huge amount of parameters that store
                                                knowledge. However, LLMs still suffer from several key issues, such as
                                                hallucination problems, knowledge update issues, and lacking
                                                domain-specific expertise. The appearance of retrieval-augmented
                                                generation (RAG), which leverages an external knowledge database to
                                                augment LLMs, makes up those drawbacks of LLMs. This paper reviews all
                                                significant techniques of RAG, especially in the retriever and the
                                                retrieval fusions. Besides, tutorial codes are provided for implementing
                                                the representative techniques in RAG. This paper further discusses the
                                                RAG training, including RAG with/without datastore update. Then, we
                                                introduce the application of RAG in representative natural language
                                                processing tasks and industrial scenarios. Finally, this paper discusses
                                                the future directions and challenges of RAG for promoting its
                                                development.
                                            </div>
                                        </div>
                                    </div>

                                </div>
                            </div>
                        </div>
                    </li>
                </ol>
            </div>
        </div>


        <h2>Conferences & Journals & Workshops</h2>

        <div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
            <div class="col-sm-1 mt-2 p-0 pr-1">
                <h3 class="bibliography-year">2025</h3>
            </div>
            <div class="col-sm-11 p-0">
                <ol class="bibliography">
                    <li>
                        <div class="row m-0 mt-3 p-0">
                            <div class="col-sm-1 p-0 abbr">
                                <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;"
                                    href="https://icml.cc/" target="_blank">
                                    ICML
                                </a>
                            </div>
                            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                                <div id="Zhou2025PromptICL" class="col p-0">
                                    <h5 class="title mb-0">Prompting Wireless Networks: Reinforced In-Context Learning for Power Control
                                    </h5>
                                    <div class="author">
                                        <nobr>Hao Zhou*,</nobr>
                                        <nobr>Chengming Hu*,</nobr>
                                        <nobr>Dun Yuan,</nobr>
                                        <nobr><em>Ye Yuan</em>,</nobr>
                                        <nobr>Di Wu,</nobr>
                                        and
                                        <nobr>Xue Liu.</nobr>

                                    </div>

                                    <div>
                                        <p class="periodical font-italic">
                                            The Forty-Second International Conference on Machine Learning (ICML 2025) ML4Wireless Workshop.
                                        </p>
                                    </div>


                                    <div class="col p-0">

                                        <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                            href="#Zhou2025PromptICL-abstarct" role="button" aria-expanded="false"
                                            aria-controls="Zhou2025PromptICL-abstarct">Abstract</a>

                                        <a class="badge grey waves-effect font-weight-light mr-1"
                                            href="/assets/pdf/telecom/2506.06526v1.pdf" target="_blank">PDF</a>
                                    </div>


                                    <div class="col mt-2 p-0">
                                        <div id="Zhou2025PromptICL-abstarct" class="collapse">
                                            <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                                To manage and optimize constantly evolving wireless networks, existing machine learning (ML)- based studies operate as black-box models, leading to increased computational costs during training and a lack of transparency in decision-making, which limits their practical applicability in wireless networks. Motivated by recent advancements in large language model (LLM)-enabled wireless networks, this paper proposes ProWin, a novel framework that leverages reinforced in-context learning to design task-specific demonstration Prompts for Wireless Network optimization, relying on the inference capabilities of LLMs without the need for dedicated model training or finetuning. The task-specific prompts are designed to incorporate natural language descriptions of the task description and formulation, enhancing interpretability and eliminating the need for specialized expertise in network optimization. We further propose a reinforced in-context learning scheme that incorporates a set of advisable examples into task-specific prompts, wherein informative examples capturing historical environment states and decisions are adaptively selected to guide current decision-making. Evaluations on a case study of base station power control showcases that the proposed ProWin outperforms reinforcement learning (RL)-based methods, highlighting the potential for next-generation future wireless network optimization.    
                                            </div>
                                        </div>
                                    </div>

                                </div>
                            </div>
                        </div>
                    </li>

                    <li>
                        <div class="row m-0 mt-3 p-0">
                            <div class="col-sm-1 p-0 abbr">
                                <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;"
                                    href="https://jmlr.org/tmlr/" target="_blank">
                                    TMLR
                                </a>
                            </div>
                            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                                <div id="Yuan2025Demo" class="col p-0">
                                    <h5 class="title mb-0">Design Editing for Offline Model-Based Optimization
                                    </h5>
                                    <div class="author">
                                        <nobr><em>Ye Yuan*</em>,</nobr>
                                        <nobr>Youyuan Zhang*,</nobr>
                                        <nobr>Can Chen,</nobr>
                                        <nobr>Haolun Wu,</nobr>
                                        <nobr>Zixuan Li,</nobr>
                                        <nobr>Jianmo Li,</nobr>
                                        <nobr>James J. Clark,</nobr>
                                        and
                                        <nobr>Xue Liu.</nobr>

                                    </div>

                                    <div>
                                        <p class="periodical font-italic">
                                            Transactions on Machine Learning Research.
                                        </p>
                                    </div>


                                    <div class="col p-0">

                                        <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                            href="#Yuan2025Demo-abstarct" role="button" aria-expanded="false"
                                            aria-controls="Yuan2025Demo-abstarct">Abstract</a>

                                        <a class="badge grey waves-effect font-weight-light mr-1"
                                            href="/assets/pdf/mbo/DEMO_2025.pdf" target="_blank">PDF</a>
                                    </div>


                                    <div class="col mt-2 p-0">
                                        <div id="Yuan2025Demo-abstarct" class="collapse">
                                            <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                                Offline model-based optimization (MBO) aims to maximize a black-box objective function using only an offline dataset of designs and scores. These tasks span various domains, such as robotics, material design, and protein and molecular engineering. A common approach involves training a surrogate model using existing designs and their corresponding scores, and then generating new designs through gradient-based updates with respect to the surrogate model. This method suffers from the out-of-distribution issue, where the surrogate model may erroneously predict high scores for unseen designs. To address this challenge, we introduce a novel method, Design Editing for Offline Model-based Optimization} (DEMO), which leverages a diffusion prior to calibrate overly optimized designs. DEMO first generates pseudo design candidates by performing gradient ascent with respect to a surrogate model. Then, an editing process refines these pseudo design candidates by introducing noise and subsequently denoising them with a diffusion prior trained on the offline dataset, ensuring they align with the distribution of valid designs. We provide a theoretical proof that the difference between the final optimized designs generated by DEMO and the prior distribution of the offline dataset is controlled by the noise injected during the editing process. Empirical evaluations on seven offline MBO tasks show that DEMO outperforms various baseline methods, achieving the highest mean rank of 2.1 and a median rank of 1.    
                                            </div>
                                        </div>
                                    </div>

                                </div>
                            </div>
                        </div>
                    </li>

                    <li>
                        <div class="row m-0 mt-3 p-0">
                            <div class="col-sm-1 p-0 abbr">
                                <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;"
                                    href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7742" target="_blank">
                                    WCM
                                </a>
                            </div>
                            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                                <div id="Zhou2025Prompt" class="col p-0">
                                    <h5 class="title mb-0">Large Language Models for Wireless Networks: An Overview from the Prompt Engineering Perspective
                                    </h5>
                                    <div class="author">
                                        <nobr>Hao Zhou,</nobr>
                                        <nobr>Chengming Hu,</nobr>
                                        <nobr>Dun Yuan,</nobr>
                                        <nobr><em>Ye Yuan</em>,</nobr>
                                        <nobr>Di Wu,</nobr>
                                        <nobr>Xi Chen,</nobr>
                                        <nobr>Hina Tabassum,</nobr>
                                        and
                                        <nobr>Xue Liu.</nobr>

                                    </div>

                                    <div>
                                        <p class="periodical font-italic">
                                            IEEE Wireless Communications.
                                        </p>
                                    </div>


                                    <div class="col p-0">

                                        <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                            href="#Zhou2025Prompt-abstarct" role="button" aria-expanded="false"
                                            aria-controls="Zhou2025Prompt-abstarct">Abstract</a>

                                        <a class="badge grey waves-effect font-weight-light mr-1"
                                            href="/assets/pdf/telecom/2411.04136v2.pdf" target="_blank">PDF</a>
                                    </div>


                                    <div class="col mt-2 p-0">
                                        <div id="Zhou2025Prompt-abstarct" class="collapse">
                                            <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                                Recently, large language models (LLMs) have been successfully applied to many fields, showing outstanding comprehension and reasoning capabilities. Despite their great potential, LLMs usually require dedicated pre-training and fine-tuning for domain-specific applications such as wireless networks. These adaptations can be extremely demanding for computational resources and datasets, while most network devices have limited computation power, and there are a limited number of high-quality networking datasets. To this end, this work explores LLM-enabled wireless networks from the prompt engineering perspective, i.e., designing prompts to guide LLMs to generate desired output without updating LLM parameters. Compared with other LLM-driven methods, prompt engineering can better align with the demands of wireless network devices, e.g., higher deployment flexibility, rapid response time, and lower requirements on computation power. In particular, this work first introduces LLM fundamentals and compares different prompting techniques such as in-context learning, chain-of-thought, and self-refinement. Then we propose two novel prompting schemes for network applications: iterative prompting for network optimization, and self-refined prompting for network prediction. The case studies show that the proposed schemes can achieve comparable performance as conventional machine learning techniques, and our proposed prompting-based methods avoid the complexity of dedicated model training and fine-tuning, which is one of the key bottlenecks of existing machine learning techniques.    
                                            </div>
                                        </div>
                                    </div>

                                </div>
                            </div>
                        </div>
                    </li>

                    <li>
                        <div class="row m-0 mt-3 p-0">
                            <div class="col-sm-1 p-0 abbr">
                                <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;"
                                    href="https://iclr.cc/" target="_blank">
                                    ICLR
                                </a>
                            </div>
                            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                                <div id="Yuan2025ICLR" class="col p-0">
                                    <h5 class="title mb-0">ParetoFlow: Guided Flows in Multi-Objective Optimization</h5>
                                    <div class="author">
                                        <nobr><em>Ye Yuan*</em>,</nobr>
                                        <nobr>Can Chen*,</nobr>
                                        <nobr>Christopher Pal,</nobr>
                                        and
                                        <nobr>Xue Liu.</nobr>
                                    </div>

                                    <div>
                                        <p class="periodical font-italic">
                                            The Thirteenth International Conference on Learning Representations (ICLR 2025).
                                            <br>
                                            <!--                                        <nobr><em style="color:orange;">Best Paper Award Winner</em></nobr>-->
                                        </p>
                                    </div>

                                    <div class="col p-0">

                                        <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                            href="#Yuan2025ICLR-abstract" role="button" aria-expanded="false"
                                            aria-controls="Yuan2025ICLR-abstract">Abstract</a>

                                        <a class="badge grey waves-effect font-weight-light mr-1"
                                            href="/assets/pdf/mbo/ParetoFlow_2025.pdf"
                                            target="_blank">PDF</a>
                                    </div>


                                    <div class="col mt-2 p-0">
                                        <div id="Yuan2025ICLR-abstract" class="collapse">
                                            <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                                In offline multi-objective optimization (MOO), we leverage an offline dataset of designs and their associated labels to simultaneously minimize multiple objectives. This setting more closely mirrors complex real-world problems compared to single-objective optimization. Recent works mainly employ evolutionary algorithms and Bayesian optimization, with limited attention given to the generative modeling capabilities inherent in such data. In this study, we explore generative modeling in offline MOO through flow matching, noted for its effectiveness and efficiency. We introduce ParetoFlow, specifically designed to guide flow sampling to approximate the Pareto front. Traditional predictor (classifier) guidance is inadequate for this purpose because it models only a single objective. In response, we propose a multi-objective predictor guidance module that assigns each sample a weight vector, representing a weighted distribution across multiple objective predictions. A local filtering scheme is introduced to address non-convex Pareto fronts. These weights uniformly cover the entire objective space, effectively directing sample generation towards the Pareto front. Since distributions with similar weights tend to generate similar samples, we introduce a neighboring evolution module to foster knowledge sharing among neighboring distributions. This module generates offspring from these distributions, and selects the most promising one for the next iteration. Our method achieves state-of-the-art performance across various tasks.
                                            </div>
                                        </div>
                                    </div>

                                </div>
                            </div>
                        </div>
                    </li>
                </ol>


                </ol>
            </div>
        </div>


        <div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
            <div class="col-sm-1 mt-2 p-0 pr-1">
                <h3 class="bibliography-year">2024</h3>
            </div>
            <div class="col-sm-11 p-0">
                <ol class="bibliography">

                    <li>
                        <div class="row m-0 mt-3 p-0">
                            <div class="col-sm-1 p-0 abbr">
                                <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;"
                                    href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962382" target="_blank">
                                    WCL
                                </a>
                            </div>
                            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                                <div id="Zhou2024ICL" class="col p-0">
                                    <h5 class="title mb-0">Generative AI as a Service in 6G Edge-Cloud: Generation Task Offloading by In-Context Learning
                                    </h5>
                                    <div class="author">
                                        <nobr>Hao Zhou,</nobr>
                                        <nobr>Chengming Hu,</nobr>
                                        <nobr>Dun Yuan,</nobr>
                                        <nobr><em>Ye Yuan</em>,</nobr>
                                        <nobr>Di Wu,</nobr>
                                        <nobr>Xue Liu,</nobr>
                                        <nobr>Zhu Han,</nobr>
                                        and
                                        <nobr>Jianzhong Zhang.</nobr>

                                    </div>

                                    <div>
                                        <p class="periodical font-italic">
                                            IEEE Wireless Communications Letters.
                                        </p>
                                    </div>


                                    <div class="col p-0">

                                        <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                            href="#Zhou2024ICL-abstarct" role="button" aria-expanded="false"
                                            aria-controls="Zhou2024ICL-abstarct">Abstract</a>

                                        <a class="badge grey waves-effect font-weight-light mr-1"
                                            href="/assets/pdf/telecom/2408.02549v3.pdf" target="_blank">PDF</a>
                                    </div>


                                    <div class="col mt-2 p-0">
                                        <div id="Zhou2024ICL-abstarct" class="collapse">
                                            <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                                Generative artificial intelligence (GAI) is a promising technique towards 6G networks, and generative foundation models such as large language models (LLMs) have attracted considerable interest from academia and industry. This letter considers a novel edge-cloud deployment of foundation models in 6G networks. Specifically, it aims to minimize the service delay of foundation models by radio resource allocation and task offloading, i.e., offloading diverse content generation tasks to proper LLMs at the network edge or cloud. In particular, we first introduce the communication system model, i.e., allocating radio resources and calculating link capacity to support generated content transmission, and then we present the LLM inference model to calculate the delay of content generation. After that, we propose a novel in-context learning method to optimize the task offloading decisions. It utilizes LLM’s inference capabilities, and avoids the difficulty of dedicated model training or fine-tuning as in conventional machine learning algorithms. Finally, the simulations demonstrate that the proposed edge-cloud deployment and in-context learning method can achieve satisfactory generation service quality without dedicated model training.
                                            </div>
                                        </div>
                                    </div>

                                </div>
                            </div>
                        </div>
                    </li>

                    <li>
                        <div class="row m-0 mt-3 p-0">
                            <div class="col-sm-1 p-0 abbr">
                                <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;"
                                    href="https://2024.emnlp.org/" target="_blank">
                                    EMNLP
                                </a>
                            </div>
                            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                                <div id="Wu2024_MuSEE" class="col p-0">
                                    <h5 class="title mb-0">Learning to Extract Structured Entities Using Language Models
                                    </h5>
                                    <div class="author">
                                        <nobr><em>Ye Yuan*</em>,</nobr>
                                        <nobr>Haolun Wu*,</nobr>
                                        <nobr>Liana Mikaelyan,</nobr>
                                        <nobr>Alexander Meulemans,</nobr>
                                        <nobr>Xue Liu,</nobr>
                                        <nobr>James Hensman,</nobr>
                                        and
                                        <nobr>Bhaskar Mitra.</nobr>

                                    </div>

                                    <div>
                                        <p class="periodical font-italic">
                                            The 2024 Conference on Empirical Methods in Natural Language Processing Main Conference (EMNLP 2024).
                                        </p>
                                        <p class="periodical font-italic"><span style="color: orangered;"><b>Oral, top
                                                    7% of
                                                    all accepted
                                                    papers.</b></span></p>

                                    </div>


                                    <div class="col p-0">

                                        <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                            href="#Wu2024_MuSEE-abstract" role="button" aria-expanded="false"
                                            aria-controls="Wu2024_MuSEE-abstract">Abstract</a>

                                        <a class="badge grey waves-effect font-weight-light mr-1"
                                            href="/assets/pdf/nlp/MuSEE_2024.pdf" target="_blank">PDF</a>

                                        <!--                                    <a class="badge grey waves-effect font-weight-light mr-1"-->
                                        <!--                                       href="/assets/pdf/lumos/bobu2020HRIpioneers_poster.pdf"-->
                                        <!--                                       target="_blank">Poster</a>-->

                                    </div>


                                    <div class="col mt-2 p-0">
                                        <div id="Wu2024_MuSEE-abstract" class="collapse">
                                            <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                                Recent advances in machine learning have significantly impacted the
                                                field
                                                of
                                                information extraction, with Large Language Models (LLMs) playing a
                                                pivotal
                                                role in extracting structured information from unstructured text. Prior
                                                works typically represent information extraction as triplet-centric and
                                                use
                                                classical metrics such as precision and recall for evaluation. We
                                                reformulate the task to be entity-centric, enabling the use of diverse
                                                metrics that can provide more insights from various perspectives. We
                                                contribute to the field by introducing Structured Entity Extraction
                                                (SEE)
                                                and proposing the Approximate Entity Set OverlaP (AESOP) metric,
                                                designed to
                                                appropriately assess model performance. Later, we introduce a new model
                                                that
                                                harnesses the power of LLMs for enhanced effectiveness and efficiency by
                                                decomposing the extraction task into multiple stages. Quantitative and
                                                human
                                                side-by-side evaluations confirm that our model outperforms baselines,
                                                offering promising directions for future advancements in structured
                                                entity
                                                extraction.
                                            </div>
                                        </div>
                                    </div>

                                </div>
                            </div>
                        </div>
                    </li>

                    <li>
                        <div class="row m-0 mt-3 p-0">
                            <div class="col-sm-1 p-0 abbr">
                                <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;"
                                    href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=9739" target="_blank">
                                    COMST
                                </a>
                            </div>
                            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                                <div id="Zhou2024_Survey" class="col p-0">
                                    <h5 class="title mb-0">Large Language Model (LLM) for Telecommunications: A Comprehensive Survey on Principles, Key Techniques, and Opportunities
                                    </h5>
                                    <div class="author">
                                        <nobr>Hao Zhou,</nobr>
                                        <nobr>Chengming Hu,</nobr>
                                        <nobr><em>Ye Yuan</em>,</nobr>
                                        <nobr>Yufei Cui,</nobr>
                                        <nobr>Yili Jin,</nobr>
                                        <nobr>Can Chen,</nobr>
                                        <nobr>Haolun Wu,</nobr>
                                        <nobr>Dun Yuan,</nobr>
                                        <nobr>Li Jiang,</nobr>
                                        <nobr>Di Wu,</nobr>
                                        <nobr>Xue Liu,</nobr>
                                        <nobr>Charlie Zhang,</nobr>
                                        <nobr>Xianbin Wang,</nobr>
                                        and
                                        <nobr>Jiangchuan Liu.</nobr>

                                    </div>

                                    <div>
                                        <p class="periodical font-italic">
                                            IEEE Communications Surveys & Tutorials.
                                        </p>
                                    </div>


                                    <div class="col p-0">

                                        <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                            href="#Zhou2024_Survey-abstarct" role="button" aria-expanded="false"
                                            aria-controls="Zhou2024_Survey-abstarct">Abstract</a>

                                        <a class="badge grey waves-effect font-weight-light mr-1"
                                            href="/assets/pdf/telecom/2405.10825v2.pdf" target="_blank">PDF</a>

                                        <!--                                    <a class="badge grey waves-effect font-weight-light mr-1"-->
                                        <!--                                       href="/assets/pdf/lumos/bobu2020HRIpioneers_poster.pdf"-->
                                        <!--                                       target="_blank">Poster</a>-->

                                    </div>


                                    <div class="col mt-2 p-0">
                                        <div id="Zhou2024_Survey-abstarct" class="collapse">
                                            <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                                Large language models (LLMs) have received considerable attention recently due to their outstanding comprehension and reasoning capabilities, leading to great progress in many fields. The advancement of LLM techniques also offers promising opportunities to automate many tasks in the telecommunication (telecom) field. After pre-training and fine-tuning, LLMs can perform diverse downstream tasks based on human instructions, paving the way to artificial general intelligence (AGI)-enabled 6G. Given the great potential of LLM technologies, this work aims to provide a comprehensive overview of LLM-enabled telecom networks. In particular, we first present LLM fundamentals, including model architecture, pre-training, fine-tuning, inference and utilization, model evaluation, and telecom deployment. Then, we introduce LLM-enabled key techniques and telecom applications in terms of generation, classification, optimization, and prediction problems. Specifically, the LLM-enabled generation applications include telecom domain knowledge, code, and network configuration generation. After that, the LLM-based classification applications involve network security, text, image, and traffic classification problems. Moreover, multiple LLM-enabled optimization techniques are introduced, such as automated reward function design for reinforcement learning and verbal reinforcement learning. Furthermore, for LLM-aided prediction problems, we discussed time-series prediction models and multi-modality prediction problems for telecom. Finally, we highlight the challenges and identify the future directions of LLM-enabled telecom networks.
                                            </div>
                                        </div>
                                    </div>

                                </div>
                            </div>
                        </div>
                    </li>

                </ol>


                </ol>
            </div>
        </div>

        <div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
            <div class="col-sm-1 mt-2 p-0 pr-1">
                <h3 class="bibliography-year">2023</h3>
            </div>
            <div class="col-sm-11 p-0">
                <ol class="bibliography">
                    <li>
                        <div class="row m-0 mt-3 p-0">
                            <div class="col-sm-1 p-0 abbr">
                                <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;"
                                    href="https://neurips.cc/Conferences/2023" target="_blank">
                                    NeurIPS
                                </a>
                            </div>
                            <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                                <div id="Yuan2023NeurIPS" class="col p-0">
                                    <h5 class="title mb-0">Importance-Aware Co-Teaching for Offline Model-Based Optimization</h5>
                                    <div class="author">
                                        <nobr><em>Ye Yuan*</em>,</nobr>
                                        <nobr>Can Chen*,</nobr>
                                        <nobr>Zixuan Liu,</nobr>
                                        <nobr>Willie Neiswanger,</nobr>
                                        and
                                        <nobr>Xue Liu.</nobr>
                                    </div>

                                    <div>
                                        <p class="periodical font-italic">
                                            Advances in Neural Information Processing Systems 36 (NeurIPS 2023).
                                            <br>
                                            <!--                                        <nobr><em style="color:orange;">Best Paper Award Winner</em></nobr>-->
                                        </p>
                                    </div>

                                    <div class="col p-0">

                                        <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                            href="#Yuan2023NeurIPS-abstract" role="button" aria-expanded="false"
                                            aria-controls="Yuan2023NeurIPS-abstract">Abstract</a>

                                        <a class="badge grey waves-effect font-weight-light mr-1"
                                            href="/assets/pdf/mbo/ICT_2023.pdf"
                                            target="_blank">PDF</a>
                                    </div>


                                    <div class="col mt-2 p-0">
                                        <div id="Yuan2023NeurIPS-abstract" class="collapse">
                                            <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                                Offline model-based optimization aims to find a design that maximizes a property of interest using only an offline dataset, with applications in robot, protein, and molecule design, among others. A prevalent approach is gradient ascent, where a proxy model is trained on the offline dataset and then used to optimize the design. This method suffers from an out-of-distribution issue, where the proxy is not accurate for unseen designs. To mitigate this issue, we explore using a pseudo-labeler to generate valuable data for fine-tuning the proxy. Specifically, we propose \textit{\textbf{I}mportance-aware \textbf{C}o-\textbf{T}eaching for Offline Model-based Optimization}~(\textbf{ICT}). This method maintains three symmetric proxies with their mean ensemble as the final proxy, and comprises two steps. The first step is \textit{pseudo-label-driven co-teaching}. In this step, one proxy is iteratively selected as the pseudo-labeler for designs near the current optimization point, generating pseudo-labeled data. Subsequently, a co-teaching process identifies small-loss samples as valuable data and exchanges them between the other two proxies for fine-tuning, promoting knowledge transfer. This procedure is repeated three times, with a different proxy chosen as the pseudo-labeler each time, ultimately enhancing the ensemble performance. To further improve accuracy of pseudo-labels, we perform a secondary step of \textit{meta-learning-based sample reweighting}, which assigns importance weights to samples in the pseudo-labeled dataset and updates them via meta-learning. ICT achieves state-of-the-art results across multiple design-bench tasks, achieving the best mean rank of 3.1 and median rank of 2, among 15 methods. Our source code can be found here.
                                            </div>
                                        </div>
                                    </div>

                                </div>
                            </div>
                        </div>
                    </li>
                </ol>


                </ol>
            </div>
        </div>

    </div>

    <!-- Footer -->
    <footer>
        &copy; Copyright 2025 Ye Yuan.


    </footer>

    <!-- Core JavaScript Files -->
    <script src="/assets/js/jquery.min.js" type="text/javascript"></script>
    <script src="/assets/js/popper.min.js" type="text/javascript"></script>
    <script src="/assets/js/bootstrap.min.js" type="text/javascript"></script>
    <script src="/assets/js/mdb.min.js" type="text/javascript"></script>
    <script async="" src="https://cdnjs.cloudflare.com/ajax/libs/masonry/4.2.2/masonry.pkgd.min.js"
        integrity="sha384-GNFwBvfVxBkLMJpYMOABq3c+d3KnQxudP/mGPkzpZSTYykLBNsZEnG2D9G/X/+7D"
        crossorigin="anonymous"></script>
    <script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
    <script src="/assets/js/common.js"></script>

    <!-- Scrolling Progress Bar -->
    <script type="text/javascript">
        $(document).ready(function () {
            var navbarHeight = $('#navbar').outerHeight(true);
            $('body').css({ 'padding-top': navbarHeight });
            $('progress-container').css({ 'padding-top': navbarHeight });
            var progressBar = $('#progress');
            progressBar.css({ 'top': navbarHeight });
            var getMax = function () {
                return $(document).height() - $(window).height();
            }
            var getValue = function () {
                return $(window).scrollTop();
            }
            // Check if the browser supports the progress element.
            if ('max' in document.createElement('progress')) {
                // Set the 'max' attribute for the first time.
                progressBar.attr({ max: getMax() });
                progressBar.attr({ value: getValue() });

                $(document).on('scroll', function () {
                    // On scroll only the 'value' attribute needs to be calculated.
                    progressBar.attr({ value: getValue() });
                });

                $(window).resize(function () {
                    var navbarHeight = $('#navbar').outerHeight(true);
                    $('body').css({ 'padding-top': navbarHeight });
                    $('progress-container').css({ 'padding-top': navbarHeight });
                    progressBar.css({ 'top': navbarHeight });
                    // On resize, both the 'max' and 'value' attributes need to be calculated.
                    progressBar.attr({ max: getMax(), value: getValue() });
                });
            } else {
                var max = getMax(), value, width;
                var getWidth = function () {
                    // Calculate the window width as a percentage.
                    value = getValue();
                    width = (value / max) * 100;
                    width = width + '%';
                    return width;
                }
                var setWidth = function () {
                    progressBar.css({ width: getWidth() });
                };
                setWidth();
                $(document).on('scroll', setWidth);
                $(window).on('resize', function () {
                    // Need to reset the 'max' attribute.
                    max = getMax();
                    setWidth();
                });
            }
        });
    </script>

    <!-- Code Syntax Highlighting -->
    <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet">
    <script src="/assets/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    <!-- Script Used for Randomizing the Projects Order -->
    <!-- <script type="text/javascript">
  $.fn.shuffleChildren = function() {
    $.each(this.get(), function(index, el) {
      var $el = $(el);
      var $find = $el.children();

      $find.sort(function() {
        return 0.5 - Math.random();
      });

      $el.empty();
      $find.appendTo($el);
    });
  };
  $("#projects").shuffleChildren();
</script> -->

    <!-- Project Cards Layout -->
    <script type="text/javascript">
        var $grid = $('#projects');

        // $grid.masonry({ percentPosition: true });
        // $grid.masonry('layout');

        // Trigger after images load.
        $grid.imagesLoaded().progress(function () {
            $grid.masonry({ percentPosition: true });
            $grid.masonry('layout');
        });
    </script>

    <!-- Enable Tooltips -->
    <script type="text/javascript">
        $(function () {
            $('[data-toggle="tooltip"]').tooltip()
        })
    </script>

    <!-- Google Analytics -->
    <script>
            (function (i, s, o, g, r, a, m) {
                i['GoogleAnalyticsObject'] = r;
                i[r] = i[r] || function () {
                    (i[r].q = i[r].q || []).push(arguments)
                }, i[r].l = 1 * new Date();
                a = s.createElement(o),
                    m = s.getElementsByTagName(o)[0];
                a.async = 1;
                a.src = g;
                m.parentNode.insertBefore(a, m)
            })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');
        ga('create', '', 'auto');
        ga('send', 'pageview');
    </script>
</body>

</html>